{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SETUP AND IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getnumber(filename):\n",
    "    match = re.search(r\"file(\\d+)\\.txt\", filename)\n",
    "    if match:\n",
    "        integer = int(match.group(1))\n",
    "        return integer\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def pre_process_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    filtered_tokens = [word for word in filtered_tokens if word.isalnum()]\n",
    "    filtered_tokens = list(filter(lambda token: token.strip() != '', filtered_tokens))\n",
    "    return filtered_tokens\n",
    "\n",
    "def pre_process_file(read_file_path , write_file_path , mode = 'w'):\n",
    "    with open(write_file_path, 'w') as write_file:\n",
    "        with open(read_file_path, 'r') as read_file:\n",
    "            content = read_file.read()\n",
    "            content_lower = content.lower()         #lowercase the content\n",
    "            tokens = word_tokenize(content_lower)   # perform tokenization\n",
    "            stop_words = set(stopwords.words('english'))    # get the stop words\n",
    "            filtered_tokens_wout_stop_words = [word for word in tokens if word.lower() not in stop_words] # remove the stop words\n",
    "            filtered_tokens_wout_punc = [word for word in filtered_tokens_wout_stop_words if word.isalnum()] # remove the punctuations\n",
    "            filtered_tokens = list(filter(lambda token: token.strip() != '', filtered_tokens_wout_punc)) # remove the empty strings\n",
    "\n",
    "            if(mode == 'p'):\n",
    "                print(\"Original Content: \")\n",
    "                print(content)\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"Lowercase Content: \")\n",
    "                print(content_lower)\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"Tokens: \")\n",
    "                print(tokens)\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"Filtered Tokens without stop words: \")\n",
    "                print(filtered_tokens_wout_stop_words)\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"Filtered Tokens without punctuations: \")\n",
    "                print(filtered_tokens_wout_punc)\n",
    "                print(\"-----------------------------------\")\n",
    "                print(\"Filtered Tokens after removal of empty spaces: \")\n",
    "                print(filtered_tokens)\n",
    "                print(\"-----------------------------------\")    \n",
    "                print()\n",
    "                print()\n",
    "            if(mode == 'w'):\n",
    "                write_file.write(' '.join(filtered_tokens)) # write the filtered tokens to the file\n",
    "\n",
    "def return_files_in_dir(directory_path):\n",
    "    input_files = []\n",
    "    with os.scandir(directory_path) as entries:\n",
    "        for entry in entries:\n",
    "            if entry.is_file():\n",
    "                input_files.append(entry.name)    # print(input_files)\n",
    "    return input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_directory_path = '/Users/utkarshpal/python_projects/IR/text_files'\n",
    "write_directory_path = '/Users/utkarshpal/python_projects/IR/new_text_files'\n",
    "# List all files in the directory\n",
    "input_files = [f for f in os.listdir(read_directory_path) if os.path.isfile(os.path.join(read_directory_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file502.txt\n",
      "Original Content: \n",
      "Kit is awesome. I play in my garage just for personal enjoyment not for performances or anything. Once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. With the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "After a few weeks of daily use for at least an hour a day it still looks and plays beautifully. Overall one of the best purchases I could have made.\n",
      "-----------------------------------\n",
      "Lowercase Content: \n",
      "kit is awesome. i play in my garage just for personal enjoyment not for performances or anything. once you take the time to break down all the settings, your able to dial in pretty much any kit and sound. with the expansion options and the relatively inexpensive parts expanding is easy and fun.\n",
      "\n",
      "after a few weeks of daily use for at least an hour a day it still looks and plays beautifully. overall one of the best purchases i could have made.\n",
      "-----------------------------------\n",
      "Tokens: \n",
      "['kit', 'is', 'awesome', '.', 'i', 'play', 'in', 'my', 'garage', 'just', 'for', 'personal', 'enjoyment', 'not', 'for', 'performances', 'or', 'anything', '.', 'once', 'you', 'take', 'the', 'time', 'to', 'break', 'down', 'all', 'the', 'settings', ',', 'your', 'able', 'to', 'dial', 'in', 'pretty', 'much', 'any', 'kit', 'and', 'sound', '.', 'with', 'the', 'expansion', 'options', 'and', 'the', 'relatively', 'inexpensive', 'parts', 'expanding', 'is', 'easy', 'and', 'fun', '.', 'after', 'a', 'few', 'weeks', 'of', 'daily', 'use', 'for', 'at', 'least', 'an', 'hour', 'a', 'day', 'it', 'still', 'looks', 'and', 'plays', 'beautifully', '.', 'overall', 'one', 'of', 'the', 'best', 'purchases', 'i', 'could', 'have', 'made', '.']\n",
      "-----------------------------------\n",
      "Filtered Tokens without stop words: \n",
      "['kit', 'awesome', '.', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', '.', 'take', 'time', 'break', 'settings', ',', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', '.', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', '.', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', '.', 'overall', 'one', 'best', 'purchases', 'could', 'made', '.']\n",
      "-----------------------------------\n",
      "Filtered Tokens without punctuations: \n",
      "['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
      "-----------------------------------\n",
      "Filtered Tokens after removal of empty spaces: \n",
      "['kit', 'awesome', 'play', 'garage', 'personal', 'enjoyment', 'performances', 'anything', 'take', 'time', 'break', 'settings', 'able', 'dial', 'pretty', 'much', 'kit', 'sound', 'expansion', 'options', 'relatively', 'inexpensive', 'parts', 'expanding', 'easy', 'fun', 'weeks', 'daily', 'use', 'least', 'hour', 'day', 'still', 'looks', 'plays', 'beautifully', 'overall', 'one', 'best', 'purchases', 'could', 'made']\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "file264.txt\n",
      "Original Content: \n",
      "I just tested this fog fluid with a 1byone 400W fogger. Two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. This being a hot space I was pleasantly surprised by how long the fog would linger. It would quickly rise to eye level and then just hang there. Another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "Only downside is that the fog is not very dense. The difference between 2 bursts and 5 bursts was not too pronounced; it's a grey mist that does not become white or truly opaque, with visibility remaining at > 9 feet.\n",
      "-----------------------------------\n",
      "Lowercase Content: \n",
      "i just tested this fog fluid with a 1byone 400w fogger. two 30 second bursts were sufficient to create enough fog layers for a moody atmosphere in a 2 car garage. this being a hot space i was pleasantly surprised by how long the fog would linger. it would quickly rise to eye level and then just hang there. another nice surprise was the odor- there is not much of it, but if you step in the middle of a thick pocket it smells like lavender (?) soap.\n",
      "only downside is that the fog is not very dense. the difference between 2 bursts and 5 bursts was not too pronounced; it's a grey mist that does not become white or truly opaque, with visibility remaining at > 9 feet.\n",
      "-----------------------------------\n",
      "Tokens: \n",
      "['i', 'just', 'tested', 'this', 'fog', 'fluid', 'with', 'a', '1byone', '400w', 'fogger', '.', 'two', '30', 'second', 'bursts', 'were', 'sufficient', 'to', 'create', 'enough', 'fog', 'layers', 'for', 'a', 'moody', 'atmosphere', 'in', 'a', '2', 'car', 'garage', '.', 'this', 'being', 'a', 'hot', 'space', 'i', 'was', 'pleasantly', 'surprised', 'by', 'how', 'long', 'the', 'fog', 'would', 'linger', '.', 'it', 'would', 'quickly', 'rise', 'to', 'eye', 'level', 'and', 'then', 'just', 'hang', 'there', '.', 'another', 'nice', 'surprise', 'was', 'the', 'odor-', 'there', 'is', 'not', 'much', 'of', 'it', ',', 'but', 'if', 'you', 'step', 'in', 'the', 'middle', 'of', 'a', 'thick', 'pocket', 'it', 'smells', 'like', 'lavender', '(', '?', ')', 'soap', '.', 'only', 'downside', 'is', 'that', 'the', 'fog', 'is', 'not', 'very', 'dense', '.', 'the', 'difference', 'between', '2', 'bursts', 'and', '5', 'bursts', 'was', 'not', 'too', 'pronounced', ';', 'it', \"'s\", 'a', 'grey', 'mist', 'that', 'does', 'not', 'become', 'white', 'or', 'truly', 'opaque', ',', 'with', 'visibility', 'remaining', 'at', '>', '9', 'feet', '.']\n",
      "-----------------------------------\n",
      "Filtered Tokens without stop words: \n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', '.', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', '.', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', '.', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', '.', 'another', 'nice', 'surprise', 'odor-', 'much', ',', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', '(', '?', ')', 'soap', '.', 'downside', 'fog', 'dense', '.', 'difference', '2', 'bursts', '5', 'bursts', 'pronounced', ';', \"'s\", 'grey', 'mist', 'become', 'white', 'truly', 'opaque', ',', 'visibility', 'remaining', '>', '9', 'feet', '.']\n",
      "-----------------------------------\n",
      "Filtered Tokens without punctuations: \n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', 'another', 'nice', 'surprise', 'much', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', 'soap', 'downside', 'fog', 'dense', 'difference', '2', 'bursts', '5', 'bursts', 'pronounced', 'grey', 'mist', 'become', 'white', 'truly', 'opaque', 'visibility', 'remaining', '9', 'feet']\n",
      "-----------------------------------\n",
      "Filtered Tokens after removal of empty spaces: \n",
      "['tested', 'fog', 'fluid', '1byone', '400w', 'fogger', 'two', '30', 'second', 'bursts', 'sufficient', 'create', 'enough', 'fog', 'layers', 'moody', 'atmosphere', '2', 'car', 'garage', 'hot', 'space', 'pleasantly', 'surprised', 'long', 'fog', 'would', 'linger', 'would', 'quickly', 'rise', 'eye', 'level', 'hang', 'another', 'nice', 'surprise', 'much', 'step', 'middle', 'thick', 'pocket', 'smells', 'like', 'lavender', 'soap', 'downside', 'fog', 'dense', 'difference', '2', 'bursts', '5', 'bursts', 'pronounced', 'grey', 'mist', 'become', 'white', 'truly', 'opaque', 'visibility', 'remaining', '9', 'feet']\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "file270.txt\n",
      "Original Content: \n",
      "Do not let the low price fool you! This is an incredible device with the free mixing software. It has been years since i tracked anything. Back then everything was Solid State and Analogue. With this Scarlett Solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "So if you are new or getting back to it, GET IT!\n",
      "\n",
      "Pro's\n",
      "Customer Support\n",
      "You Tube videos\n",
      "\n",
      "Con's\n",
      "TOO many features with the Scarlet Pro Software.\n",
      "A instructional, \"downloadable\" simple user manual.\n",
      "-----------------------------------\n",
      "Lowercase Content: \n",
      "do not let the low price fool you! this is an incredible device with the free mixing software. it has been years since i tracked anything. back then everything was solid state and analogue. with this scarlett solo, a moderate computer, and a large monitor (i stress large because there is too much control with the software), you can really lay down some serious tracks.\n",
      "\n",
      "so if you are new or getting back to it, get it!\n",
      "\n",
      "pro's\n",
      "customer support\n",
      "you tube videos\n",
      "\n",
      "con's\n",
      "too many features with the scarlet pro software.\n",
      "a instructional, \"downloadable\" simple user manual.\n",
      "-----------------------------------\n",
      "Tokens: \n",
      "['do', 'not', 'let', 'the', 'low', 'price', 'fool', 'you', '!', 'this', 'is', 'an', 'incredible', 'device', 'with', 'the', 'free', 'mixing', 'software', '.', 'it', 'has', 'been', 'years', 'since', 'i', 'tracked', 'anything', '.', 'back', 'then', 'everything', 'was', 'solid', 'state', 'and', 'analogue', '.', 'with', 'this', 'scarlett', 'solo', ',', 'a', 'moderate', 'computer', ',', 'and', 'a', 'large', 'monitor', '(', 'i', 'stress', 'large', 'because', 'there', 'is', 'too', 'much', 'control', 'with', 'the', 'software', ')', ',', 'you', 'can', 'really', 'lay', 'down', 'some', 'serious', 'tracks', '.', 'so', 'if', 'you', 'are', 'new', 'or', 'getting', 'back', 'to', 'it', ',', 'get', 'it', '!', \"pro's\", 'customer', 'support', 'you', 'tube', 'videos', \"con's\", 'too', 'many', 'features', 'with', 'the', 'scarlet', 'pro', 'software', '.', 'a', 'instructional', ',', '``', 'downloadable', \"''\", 'simple', 'user', 'manual', '.']\n",
      "-----------------------------------\n",
      "Filtered Tokens without stop words: \n",
      "['let', 'low', 'price', 'fool', '!', 'incredible', 'device', 'free', 'mixing', 'software', '.', 'years', 'since', 'tracked', 'anything', '.', 'back', 'everything', 'solid', 'state', 'analogue', '.', 'scarlett', 'solo', ',', 'moderate', 'computer', ',', 'large', 'monitor', '(', 'stress', 'large', 'much', 'control', 'software', ')', ',', 'really', 'lay', 'serious', 'tracks', '.', 'new', 'getting', 'back', ',', 'get', '!', \"pro's\", 'customer', 'support', 'tube', 'videos', \"con's\", 'many', 'features', 'scarlet', 'pro', 'software', '.', 'instructional', ',', '``', 'downloadable', \"''\", 'simple', 'user', 'manual', '.']\n",
      "-----------------------------------\n",
      "Filtered Tokens without punctuations: \n",
      "['let', 'low', 'price', 'fool', 'incredible', 'device', 'free', 'mixing', 'software', 'years', 'since', 'tracked', 'anything', 'back', 'everything', 'solid', 'state', 'analogue', 'scarlett', 'solo', 'moderate', 'computer', 'large', 'monitor', 'stress', 'large', 'much', 'control', 'software', 'really', 'lay', 'serious', 'tracks', 'new', 'getting', 'back', 'get', 'customer', 'support', 'tube', 'videos', 'many', 'features', 'scarlet', 'pro', 'software', 'instructional', 'downloadable', 'simple', 'user', 'manual']\n",
      "-----------------------------------\n",
      "Filtered Tokens after removal of empty spaces: \n",
      "['let', 'low', 'price', 'fool', 'incredible', 'device', 'free', 'mixing', 'software', 'years', 'since', 'tracked', 'anything', 'back', 'everything', 'solid', 'state', 'analogue', 'scarlett', 'solo', 'moderate', 'computer', 'large', 'monitor', 'stress', 'large', 'much', 'control', 'software', 'really', 'lay', 'serious', 'tracks', 'new', 'getting', 'back', 'get', 'customer', 'support', 'tube', 'videos', 'many', 'features', 'scarlet', 'pro', 'software', 'instructional', 'downloadable', 'simple', 'user', 'manual']\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "file516.txt\n",
      "Original Content: \n",
      "I'm using several of these on a wall I built. The hangers work great with several different head-stock shapes. I wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone California. I ended up using velcro strips under the hangers that affix over the fretboard. Otherwise a great hanger..\n",
      "-----------------------------------\n",
      "Lowercase Content: \n",
      "i'm using several of these on a wall i built. the hangers work great with several different head-stock shapes. i wish there were available bands, straps or locking devices to keep instruments secure here in earthquake prone california. i ended up using velcro strips under the hangers that affix over the fretboard. otherwise a great hanger..\n",
      "-----------------------------------\n",
      "Tokens: \n",
      "['i', \"'m\", 'using', 'several', 'of', 'these', 'on', 'a', 'wall', 'i', 'built', '.', 'the', 'hangers', 'work', 'great', 'with', 'several', 'different', 'head-stock', 'shapes', '.', 'i', 'wish', 'there', 'were', 'available', 'bands', ',', 'straps', 'or', 'locking', 'devices', 'to', 'keep', 'instruments', 'secure', 'here', 'in', 'earthquake', 'prone', 'california', '.', 'i', 'ended', 'up', 'using', 'velcro', 'strips', 'under', 'the', 'hangers', 'that', 'affix', 'over', 'the', 'fretboard', '.', 'otherwise', 'a', 'great', 'hanger', '..']\n",
      "-----------------------------------\n",
      "Filtered Tokens without stop words: \n",
      "[\"'m\", 'using', 'several', 'wall', 'built', '.', 'hangers', 'work', 'great', 'several', 'different', 'head-stock', 'shapes', '.', 'wish', 'available', 'bands', ',', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', '.', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', '.', 'otherwise', 'great', 'hanger', '..']\n",
      "-----------------------------------\n",
      "Filtered Tokens without punctuations: \n",
      "['using', 'several', 'wall', 'built', 'hangers', 'work', 'great', 'several', 'different', 'shapes', 'wish', 'available', 'bands', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', 'otherwise', 'great', 'hanger']\n",
      "-----------------------------------\n",
      "Filtered Tokens after removal of empty spaces: \n",
      "['using', 'several', 'wall', 'built', 'hangers', 'work', 'great', 'several', 'different', 'shapes', 'wish', 'available', 'bands', 'straps', 'locking', 'devices', 'keep', 'instruments', 'secure', 'earthquake', 'prone', 'california', 'ended', 'using', 'velcro', 'strips', 'hangers', 'affix', 'fretboard', 'otherwise', 'great', 'hanger']\n",
      "-----------------------------------\n",
      "\n",
      "\n",
      "file258.txt\n",
      "Original Content: \n",
      "Poor design doesn't line their own pedals up when connected using these. The offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. As you can see in the picture, the third pedal from the chain is already almost off the board. How can they overlook this?\n",
      "-----------------------------------\n",
      "Lowercase Content: \n",
      "poor design doesn't line their own pedals up when connected using these. the offset isn't enough and results in each pedal along the line mounted a bit lower than the one next to it. as you can see in the picture, the third pedal from the chain is already almost off the board. how can they overlook this?\n",
      "-----------------------------------\n",
      "Tokens: \n",
      "['poor', 'design', 'does', \"n't\", 'line', 'their', 'own', 'pedals', 'up', 'when', 'connected', 'using', 'these', '.', 'the', 'offset', 'is', \"n't\", 'enough', 'and', 'results', 'in', 'each', 'pedal', 'along', 'the', 'line', 'mounted', 'a', 'bit', 'lower', 'than', 'the', 'one', 'next', 'to', 'it', '.', 'as', 'you', 'can', 'see', 'in', 'the', 'picture', ',', 'the', 'third', 'pedal', 'from', 'the', 'chain', 'is', 'already', 'almost', 'off', 'the', 'board', '.', 'how', 'can', 'they', 'overlook', 'this', '?']\n",
      "-----------------------------------\n",
      "Filtered Tokens without stop words: \n",
      "['poor', 'design', \"n't\", 'line', 'pedals', 'connected', 'using', '.', 'offset', \"n't\", 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', '.', 'see', 'picture', ',', 'third', 'pedal', 'chain', 'already', 'almost', 'board', '.', 'overlook', '?']\n",
      "-----------------------------------\n",
      "Filtered Tokens without punctuations: \n",
      "['poor', 'design', 'line', 'pedals', 'connected', 'using', 'offset', 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', 'see', 'picture', 'third', 'pedal', 'chain', 'already', 'almost', 'board', 'overlook']\n",
      "-----------------------------------\n",
      "Filtered Tokens after removal of empty spaces: \n",
      "['poor', 'design', 'line', 'pedals', 'connected', 'using', 'offset', 'enough', 'results', 'pedal', 'along', 'line', 'mounted', 'bit', 'lower', 'one', 'next', 'see', 'picture', 'third', 'pedal', 'chain', 'already', 'almost', 'board', 'overlook']\n",
      "-----------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file in input_files[0:5]:\n",
    "        print(file)\n",
    "        pre_process_file(read_directory_path+'/'+str(file) , write_directory_path + '/'+str(file) , 'p')\n",
    "\n",
    "for file in input_files:\n",
    "    pre_process_file(read_directory_path+'/'+str(file) , write_directory_path + '/'+str(file) , 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ungigram_inverted_index(read_directory_path):\n",
    "    input_files = return_files_in_dir(read_directory_path)\n",
    "    inverted_index = {}\n",
    "    for file_name in input_files:\n",
    "        if file_name.endswith('.txt'):  # Check if the file has a .txt extension\n",
    "            read_file_path = os.path.join(read_directory_path, file_name)\n",
    "            with open(read_file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                tokens = content.split()\n",
    "                for token in tokens:\n",
    "                    if token not in inverted_index:\n",
    "                        inverted_index[token] = [getnumber(file_name)]\n",
    "                    else:\n",
    "                        if getnumber(file_name) not in inverted_index[token]:\n",
    "                            inverted_index[token].append(getnumber(file_name))\n",
    "    with open('inverted_index.pickle', 'wb') as handle:\n",
    "        pickle.dump(inverted_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return inverted_index\n",
    "\n",
    "create_ungigram_inverted_index(\"/Users/utkarshpal/python_projects/IR/new_text_files\")\n",
    "load_inverted_index = pickle.load(open('inverted_index.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_list, operation_list, inverted_index):\n",
    "    sentence_query=\"\"\n",
    "    if len(query_list) == 1 :\n",
    "        sentence_query = query_list[0]\n",
    "        if query_list[0] in inverted_index:\n",
    "            return inverted_index[query_list[0]],sentence_query\n",
    "        else:\n",
    "            return [],sentence_query\n",
    "    else:\n",
    "        result = []\n",
    "        for i in range(len(query_list)-1):\n",
    "            if i == 0:\n",
    "                sentence_query = query_list[i] + \" \" + operation_list[i] + \" \" + query_list[i+1]\n",
    "                if operation_list[i] == 'AND':\n",
    "                    result = T1_AND_T2(query_list[i], query_list[i + 1], inverted_index)\n",
    "                elif operation_list[i] == 'OR':\n",
    "                    result = T1_OR_T2(query_list[i], query_list[i + 1], inverted_index)\n",
    "                elif operation_list[i] == 'AND NOT':\n",
    "                    result = T1_AND_NOT_T2(query_list[i], query_list[i + 1], inverted_index)\n",
    "                elif operation_list[i] == 'OR NOT':\n",
    "                    result = T1_OR_NOT_T2(query_list[i], query_list[i + 1], inverted_index)\n",
    "            else:\n",
    "                sentence_query = sentence_query + \" \" + operation_list[i] + \" \" + query_list[i+1]\n",
    "                if operation_list[i] == 'AND':\n",
    "                    result = T1_AND_T2(result, query_list[i+1], inverted_index)\n",
    "                elif operation_list[i] == 'OR':\n",
    "                    result = T1_OR_T2(result, query_list[i+1], inverted_index)\n",
    "                elif operation_list[i] == 'AND NOT':\n",
    "                    result = T1_AND_NOT_T2(result, query_list[i+1], inverted_index)\n",
    "                elif operation_list[i] == 'OR NOT':\n",
    "                    result = T1_OR_NOT_T2(result, query_list[i+1], inverted_index)\n",
    "    \n",
    "        return result , sentence_query\n",
    "\n",
    "def T1_AND_T2(term1 , term2 , inverted_index):\n",
    "    # print(term1)\n",
    "    # print(term2)\n",
    "    if type(term1) == str:\n",
    "        if term1 in inverted_index:\n",
    "            term1_set = (inverted_index[term1])\n",
    "        else:\n",
    "            term1_set = []\n",
    "    else:\n",
    "        term1_set = (term1)\n",
    "    \n",
    "    if type(term2) == str:\n",
    "        if term2 in inverted_index:\n",
    "            term2_set = (inverted_index[term2])\n",
    "        else:\n",
    "            term2_set = []\n",
    "    return list(set(term1_set).intersection(set(term2_set)))\n",
    " \n",
    "def T1_OR_T2(term1 , term2 , inverted_index):\n",
    "    if type(term1) == str:\n",
    "        if term1 in inverted_index:\n",
    "            term1_set = (inverted_index[term1])\n",
    "        else:\n",
    "            term1_set = []\n",
    "    else:\n",
    "        term1_set = (term1)\n",
    "    \n",
    "    if type(term2) == str:\n",
    "        if term2 in inverted_index:\n",
    "            term2_set = (inverted_index[term2])\n",
    "        else:\n",
    "            term2_set = []\n",
    "\n",
    "    return list(set(term1_set).union(set(term2_set)))\n",
    "\n",
    "def T1_AND_NOT_T2(term1 , term2 , inverted_index):\n",
    "    if type(term1) == str:\n",
    "        if term1 in inverted_index:\n",
    "            term1_set = (inverted_index[term1])\n",
    "        else:\n",
    "            term1_set = []\n",
    "    else:\n",
    "        term1_set = (term1)\n",
    "    \n",
    "    if type(term2) == str:\n",
    "        if term2 in inverted_index:\n",
    "            term2_set = (inverted_index[term2])\n",
    "        else:\n",
    "            term2_set = []\n",
    "    return list(set(term1_set).difference(set(term2_set)))\n",
    "\n",
    "def T1_OR_NOT_T2(term1 , term2 , inverted_index):\n",
    "    if type(term1) == str:\n",
    "        if term1 in inverted_index:\n",
    "            term1_set = (inverted_index[term1])\n",
    "        else:\n",
    "            term1_set = []\n",
    "    else:\n",
    "        term1_set = (term1)\n",
    "    \n",
    "    if type(term2) == str:\n",
    "        if term2 in inverted_index:\n",
    "            term2_set = (inverted_index[term2])\n",
    "        else:\n",
    "            term2_set = []\n",
    "    universal_set =  (set(sum(inverted_index.values(), [])))\n",
    "    not_term2_set = (universal_set.difference(set(term2_set)))\n",
    "    result_set = not_term2_set.union(set(term1_set))\n",
    "    return list(result_set)\n",
    "\n",
    "\n",
    "# def T1_OR_NOT_T2(term1 , term2 , inverted_index):\n",
    "    # if type(term1) == str:\n",
    "    #     if term1 in inverted_index:\n",
    "    #         term1_set = (inverted_index[term1])\n",
    "    #     else:\n",
    "    #         term1_set = []\n",
    "    # else:\n",
    "    #     term1_set = (term1)\n",
    "    \n",
    "    # if type(term2) == str:\n",
    "    #     if term2 in inverted_index:\n",
    "    #         term2_set = (inverted_index[term2])\n",
    "    #     else:\n",
    "    #         term2_set = []\n",
    "    # union_set = set(term1_set).union(set(term2_set))\n",
    "    # result_set = union_set.difference(set(term2_set))\n",
    "    # return list(result_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: car AND guitar\n",
      "Number of documents retrieved for query 1: [166, 174, 542]\n",
      "Names of documents retrieved for query 1: ['file166.txt', 'file174.txt', 'file542.txt']\n"
     ]
    }
   ],
   "source": [
    "def q4_query():\n",
    "    n = int(input(\"Enter the number of queries: \"))\n",
    "    input_query_list = []\n",
    "    input_operation_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        query = input(\"Enter the query: \")\n",
    "        input_query_list.append(query)\n",
    "        operation = input(\"Enter the operation: \")\n",
    "        input_operation_list.append(operation)\n",
    "    \n",
    "    for i in range(n):\n",
    "        query = input_query_list[i]\n",
    "        operation = input_operation_list[i]\n",
    "        query_list = pre_process_sentence(query)\n",
    "        operation_list = [i.strip() for i in operation.split(',')]\n",
    "        search_result,query_sentence = search(query_list, operation_list, load_inverted_index)\n",
    "        search_result.sort()\n",
    "        print(f\"Query {i+1}:\" , query_sentence)\n",
    "        print(f\"Number of documents retrieved for query {i+1}:\" , search_result)\n",
    "        print(f\"Names of documents retrieved for query {i+1}:\" , [\"file\" + str(i) + \".txt\" for i in search_result])\n",
    "    \n",
    "q4_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unigram_positional_index(read_directory_path):\n",
    "    input_files = return_files_in_dir(read_directory_path)\n",
    "\n",
    "    positional_index = {}\n",
    "    for file_name in input_files:\n",
    "        if file_name.endswith('.txt'): \n",
    "            read_file_path = os.path.join(read_directory_path, file_name)\n",
    "            with open(read_file_path, 'r') as file:\n",
    "                content = file.read()\n",
    "                tokens = content.split()\n",
    "                for i in range(len(tokens)):\n",
    "                    if tokens[i] not in positional_index:\n",
    "                        positional_index[tokens[i]] = {getnumber(file_name):[i]}\n",
    "                    else:\n",
    "                        if getnumber(file_name) not in positional_index[tokens[i]]:\n",
    "                            positional_index[tokens[i]][getnumber(file_name)] = [i]\n",
    "                        else:\n",
    "                            positional_index[tokens[i]][getnumber(file_name)].append(i)\n",
    "    new_positional_index = {}\n",
    "    for key in positional_index:\n",
    "        new_positional_index[key] = list(sorted(positional_index[key].items()))\n",
    "\n",
    "    with open('positional_index.pickle', 'wb') as handle:\n",
    "        pickle.dump(new_positional_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return new_positional_index\n",
    "\n",
    "create_unigram_positional_index(\"/Users/utkarshpal/python_projects/IR/new_text_files\")\n",
    "load_positional_index = pickle.load(open('positional_index.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_intersect(term1_list,term2_list,diff):\n",
    "    result = {}\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(term1_list) and j < len(term2_list):\n",
    "        if term1_list[i][0] == term2_list[j][0]:\n",
    "            term1_positions = term1_list[i][1]\n",
    "            term2_positions = term2_list[j][1]\n",
    "            for term1_position in term1_positions:\n",
    "                for term2_position in term2_positions:\n",
    "                    if term2_position - term1_position == diff:\n",
    "                        if term1_list[i][0] not in result:\n",
    "                            result[term1_list[i][0]] = [term2_position]\n",
    "                        else:\n",
    "                            result[term1_list[i][0]].append(term2_position)\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif term1_list[i][0] < term2_list[j][0]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "    result = list(result.items())     \n",
    "    return result\n",
    "\n",
    "\n",
    "def positional_search(query_list, positional_index):\n",
    "    result = []\n",
    "    if len(query_list) == 1:\n",
    "        return positional_index[query_list[0][0]]\n",
    "    \n",
    "    for i in range(len(query_list)-1):\n",
    "        if query_list[i][0] in positional_index:\n",
    "            list1 = positional_index[query_list[i][0]]\n",
    "        else:\n",
    "            list1 = []\n",
    "        if query_list[i+1][0] in positional_index:\n",
    "            list2 = positional_index[query_list[i+1][0]]\n",
    "        else:\n",
    "            list2 = []\n",
    "            \n",
    "        if i == 0:\n",
    "            result = positional_intersect(list1,list2,query_list[i+1][1]-query_list[i][1])\n",
    "        else:\n",
    "            result = positional_intersect(result, list2,query_list[i+1][1]-query_list[i][1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1: highly recommended\n",
      "Number of documents retrieved for query 1: 6\n",
      "Number of documents retrieved for query 1: [(50, [34]), (81, [30]), (171, [10]), (257, [65]), (382, [64]), (626, [64])]\n",
      "Names of documents retrieved for query 1: ['file50.txt', 'file81.txt', 'file171.txt', 'file257.txt', 'file382.txt', 'file626.txt']\n"
     ]
    }
   ],
   "source": [
    "def q5_query():\n",
    "    n = int(input(\"Enter the number of queries: \"))\n",
    "    input_query_list = []\n",
    "    input_operation_list = []\n",
    "\n",
    "    for i in range(n):\n",
    "        query = input(\"Enter the query: \")\n",
    "        input_query_list.append(query.lower())\n",
    "    \n",
    "    for i in range(n):\n",
    "        query = input_query_list[i] \n",
    "        query_list = pre_process_sentence(query)\n",
    "        query_list_index = [[query_list[i].strip() ,i] for i in range(len(query_list))]\n",
    "\n",
    "        result = positional_search(query_list_index, load_positional_index)\n",
    "        print(f\"Query {i+1}:\" , query)\n",
    "        print(f\"Number of documents retrieved for query {i+1}:\" , len(result))\n",
    "        print(f\"Number of documents retrieved for query {i+1}:\" , (result))\n",
    "        print(f\"Names of documents retrieved for query {i+1}:\" , [\"file\" + str(i[0]) + \".txt\" for i in result])\n",
    "\n",
    "q5_query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
